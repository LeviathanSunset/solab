#!/usr/bin/env python3
"""
ä»£å¸æŒæœ‰è€…åˆ†ææ¨¡å—
Token Holder Analysis Module

è¾“å…¥: ä»£å¸åˆçº¦åœ°å€
åŠŸèƒ½: åˆ†æä»£å¸çš„é¡¶çº§æŒæœ‰è€…
- è·å–è¯¥ä»£å¸çš„æ‰€æœ‰æŒæœ‰è€…åœ°å€
- å¯¹æ‰€æœ‰æŒæœ‰è€…çš„æŒä»“è¿›è¡Œå»é‡å’ŒåŸºæœ¬ä¿¡æ¯è·å–
- è¿›è¡Œä»£å¸æŒä»“æ’åï¼ˆå…±åŒæŒæœ‰äººæ•°ï¼Œæ€»ä»·å€¼ï¼Œ>=3äººï¼‰
- ç»Ÿè®¡ç›®æ ‡ä»£å¸åœ¨æ‰€æœ‰ç›®æ ‡é’±åŒ…ä¸­çš„æ’åï¼ˆ1:x,2:y...ä¸­ä½æ•°ï¼‰
- é›†ç¾¤åˆ†æï¼šé›†ç¾¤åˆ†æ•°=å…±åŒæŒä»“äººæ•°*å…±åŒä»£å¸äººæ•°ï¼ˆå»é™¤SOLå’Œä¸»æµç¨³å®šå¸åˆçº¦åœ°å€ï¼‰

Input: token contract address
Function: analyze token top holders
- Get all holder addresses for the token
- Deduplicate and get basic information for all holder positions
- Perform token holding ranking (common holder count, total value, >=3 people)
- Calculate target token ranking in all target wallets (1:x, 2:y...median)
- Cluster analysis: cluster score = common holders count * common tokens count (excluding SOL and mainstream stablecoin addresses)
"""

import sys
import os
import json
import yaml
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from collections import defaultdict, Counter
import statistics

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from crawlers.okxdex.tokenTopHolders import SimpleOKXCrawler
from crawlers.okxdex.addressBalance import OKXAddressBalanceCrawler
from crawlers.jupiter.multiTokenProfiles import JupiterTokenCrawler
from functions.models import Address, TokenBalance
from settings.config_manager import ConfigManager


class TokenHolderAnalyzer:
    """ä»£å¸æŒæœ‰è€…åˆ†æå™¨"""
    
    def __init__(self, performance_mode: str = 'high_speed'):
        """åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            performance_mode: æ€§èƒ½æ¨¡å¼
                - 'conservative': ä¿å®ˆæ¨¡å¼ï¼Œæœ€ç¨³å®š (1.2 åœ°å€/ç§’)
                - 'balanced': å¹³è¡¡æ¨¡å¼ï¼Œæ¨èä½¿ç”¨ (2.6 åœ°å€/ç§’) 
                - 'high_speed': é«˜é€Ÿæ¨¡å¼ï¼Œæé™é€Ÿåº¦ (3.4 åœ°å€/ç§’)
                - 'lightweight': è½»é‡æ¨¡å¼ï¼Œé€‚åˆç½‘ç»œä¸ä½³ (2.0 åœ°å€/ç§’)
        """
        self.performance_mode = performance_mode
        self.config = ConfigManager()
        self.holder_crawler = SimpleOKXCrawler()
        self.balance_crawler = OKXAddressBalanceCrawler(performance_mode=performance_mode)
        self.jupiter_crawler = JupiterTokenCrawler()
        
        # ä»£å¸ä¿¡æ¯ç¼“å­˜
        self.token_info_cache = {}
        
        # ä¸»æµç¨³å®šå¸å’ŒSOLåœ°å€ï¼Œç”¨äºé›†ç¾¤åˆ†ææ—¶æ’é™¤
        self.excluded_tokens = {
            "So11111111111111111111111111111111111111112",  # SOL
            "EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v",  # USDC
            "Es9vMFrzaCERmJfrF4H2FYD4KCoNkY11McCe8BenwNYB",  # USDT
            "4k3Dyjzvzp8eMZWUXbBCjEvwSkkk59S5iCNLY3QrkX6R",  # RAY
            "SRMuApVNdxXokk5GT7XD5cUUgXMBCoAz2LHeuAoKWRt",   # SRM
        }
    
    def set_auth(self, cookie: str, fp_token: str, verify_sign: str, 
                 verify_token: str, dev_id: str, site_info: str):
        """è®¾ç½®è®¤è¯ä¿¡æ¯"""
        self.holder_crawler.set_auth(cookie, fp_token, verify_sign, 
                                   verify_token, dev_id, site_info)
        self.balance_crawler.set_auth(cookie, fp_token, verify_sign, 
                                    verify_token, dev_id, site_info)
    
    def analyze_token_holders(self, token_address: str, 
                            chain_id: str = "501", 
                            max_addresses: int = None) -> Dict[str, Any]:
        """
        åˆ†æä»£å¸æŒæœ‰è€…
        
        Args:
            token_address: ä»£å¸åˆçº¦åœ°å€
            chain_id: é“¾IDï¼Œé»˜è®¤ä¸ºSolana(501)
            max_addresses: æœ€å¤§åˆ†æåœ°å€æ•°ï¼ŒNoneè¡¨ç¤ºåˆ†ææ‰€æœ‰åœ°å€
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        print(f"ğŸš€ å¼€å§‹åˆ†æä»£å¸æŒæœ‰è€…: {token_address}")
        
        # 1. è·å–ä»£å¸æŒæœ‰è€…
        holders = self.holder_crawler.get_holders(chain_id, token_address)
        if not holders:
            return {"error": "æ— æ³•è·å–æŒæœ‰è€…æ•°æ®"}
        
        print(f"âœ… è·å–åˆ° {len(holders)} ä¸ªæŒæœ‰è€…")
        
        # 2. ç­›é€‰çœŸäººåœ°å€
        human_holders = [h for h in holders if h["tag"] == "human"]
        print(f"ğŸ‘¤ çœŸäººæŒæœ‰è€…: {len(human_holders)} ä¸ª")
        
        if len(human_holders) < 3:
            return {"error": "çœŸäººæŒæœ‰è€…æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆåˆ†æ"}
        
        # 3. è·å–æ¯ä¸ªçœŸäººåœ°å€çš„è¯¦ç»†èµ„äº§ä¿¡æ¯ - ä½¿ç”¨é«˜é€Ÿæ‰¹é‡æ¨¡å¼
        print(f"ğŸ“Š æ­£åœ¨é«˜é€Ÿæ‰¹é‡è·å– {len(human_holders)} ä¸ªåœ°å€çš„èµ„äº§ä¿¡æ¯...")
        holder_profiles = []
        
        # ä½¿ç”¨é«˜é€Ÿæ‰¹é‡çˆ¬å–
        addresses = [holder['address'] for holder in human_holders]
        
        # æ‰¹é‡è·å–æ‰€æœ‰åœ°å€çš„èµ„äº§ä¿¡æ¯
        address_results = self.balance_crawler.fetch_multiple_addresses_fast(
            addresses, 
            max_workers=3,  # é™ä½å¹¶å‘æ•°æé«˜æˆåŠŸç‡
            timeout_per_request=5.0,  # å¢åŠ è¶…æ—¶æ—¶é—´
            debug=False
        )
        
        # å¤„ç†ç»“æœ
        for holder in human_holders:
            address = holder['address']
            profile = address_results.get(address)
            
            if profile and profile.balances:
                # æ£€æŸ¥æ˜¯å¦åœ¨å‰20èµ„äº§ä¸­æ‰¾åˆ°ç›®æ ‡ä»£å¸
                target_token_found = False
                target_balance_in_top20 = "0"
                target_value_in_top20 = "0"
                
                for balance in profile.balances:
                    if balance.token_contract_address == token_address:
                        target_token_found = True
                        target_balance_in_top20 = balance.amount
                        target_value_in_top20 = balance.value
                        break
                
                holder_profiles.append({
                    "address": holder["address"],
                    "target_token_balance": holder["balance"],  # å®é™…æŒæœ‰é‡
                    "target_token_value": holder["value_usd"],  # å®é™…ä»·å€¼
                    "target_in_top20": target_token_found,  # æ˜¯å¦åœ¨å‰20èµ„äº§ä¸­
                    "target_balance_top20": target_balance_in_top20,  # å‰20ä¸­çš„ä½™é¢
                    "target_value_top20": target_value_in_top20,  # å‰20ä¸­çš„ä»·å€¼
                    "profile": profile
                })
            else:
                print(f"    âš ï¸ è·å– {holder['address']} èµ„äº§å¤±è´¥æˆ–æ— èµ„äº§")
        
        print(f"âœ… æˆåŠŸè·å– {len(holder_profiles)} ä¸ªåœ°å€çš„è¯¦ç»†èµ„äº§")
        
        # 4. è¿›è¡Œå„ç§åˆ†æ
        # åˆ›å»ºè¯¦ç»†åˆ†ææ•°æ®
        detailed_analysis = {}
        for profile in holder_profiles:
            addr = profile["address"]
            detailed_analysis[addr] = {
                "target_in_top20": profile["target_in_top20"],
                "target_balance": profile["target_token_balance"],
                "target_value": profile["target_token_value"],
                "target_balance_top20": profile["target_balance_top20"],
                "target_value_top20": profile["target_value_top20"]
            }
        
        analysis_result = {
            "token_address": token_address,
            "analysis_time": datetime.now().isoformat(),
            "total_holders": len(holders),
            "human_holders": len(human_holders),
            "analyzed_addresses": len(holder_profiles),
            "holder_stats": self._analyze_holder_stats(holders),
            "common_holdings": self._analyze_common_holdings(holder_profiles, token_address),
            "target_token_ranking": self._analyze_target_token_ranking(holder_profiles, token_address),
            "cluster_analysis": self._analyze_clusters(holder_profiles),
            "detailed_analysis": detailed_analysis,  # æ·»åŠ è¯¦ç»†åˆ†ææ•°æ®
            "detailed_holders": holder_profiles,
            "all_holders": holders  # æ·»åŠ åŸå§‹æŒæœ‰è€…æ•°æ®
        }
        
        return analysis_result
    
    def _analyze_holder_stats(self, holders: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææŒæœ‰è€…ç»Ÿè®¡ä¿¡æ¯"""
        stats = {"human": 0, "pool": 0, "exchange": 0, "contract": 0}
        total_value = 0
        human_values = []
        
        for holder in holders:
            stats[holder["tag"]] += 1
            if holder["tag"] == "human":
                try:
                    value = float(holder["value_usd"])
                    human_values.append(value)
                    total_value += value
                except:
                    pass
        
        return {
            "by_type": stats,
            "human_total_value": total_value,
            "human_avg_value": total_value / stats["human"] if stats["human"] > 0 else 0,
            "human_median_value": statistics.median(human_values) if human_values else 0
        }
    
    def _get_token_info(self, token_addresses: List[str]) -> Dict[str, Dict]:
        """è·å–ä»£å¸å®Œæ•´ä¿¡æ¯"""
        print(f"ğŸª™ æ­£åœ¨è·å– {len(token_addresses)} ä¸ªä»£å¸çš„å®Œæ•´ä¿¡æ¯...")
        
        # æ£€æŸ¥ç¼“å­˜
        uncached_addresses = []
        for addr in token_addresses:
            if addr not in self.token_info_cache:
                uncached_addresses.append(addr)
        
        # æ‰¹é‡è·å–æœªç¼“å­˜çš„ä»£å¸ä¿¡æ¯
        if uncached_addresses:
            try:
                tokens = self.jupiter_crawler.get_token_info(uncached_addresses)
                for token in tokens:
                    self.token_info_cache[token.contract_address] = {
                        'symbol': token.symbol,
                        'name': token.name,
                        'decimals': token.decimals
                    }
                print(f"âœ… æˆåŠŸè·å– {len(tokens)} ä¸ªä»£å¸ä¿¡æ¯")
            except Exception as e:
                print(f"âš ï¸ è·å–ä»£å¸ä¿¡æ¯å¤±è´¥: {e}")
        
        # ä¸ºæœªæ‰¾åˆ°ä¿¡æ¯çš„ä»£å¸åˆ›å»ºé»˜è®¤ä¿¡æ¯
        result = {}
        for addr in token_addresses:
            if addr in self.token_info_cache:
                result[addr] = self.token_info_cache[addr]
            else:
                # ä½¿ç”¨åœ°å€ä½œä¸ºé»˜è®¤å€¼
                result[addr] = {
                    'symbol': f"{addr[:6]}...{addr[-4:]}",
                    'name': f"Unknown Token ({addr[:6]}...)",
                    'decimals': 6
                }
        
        return result

    def _analyze_common_holdings(self, holder_profiles: List[Dict], 
                               target_token: str) -> Dict[str, Any]:
        """åˆ†æå…±åŒæŒä»“æƒ…å†µ"""
        # ç»Ÿè®¡æ¯ä¸ªä»£å¸è¢«å¤šå°‘ä¸ªåœ°å€æŒæœ‰
        token_holders = defaultdict(list)
        
        for profile in holder_profiles:
            address = profile["address"]
            for balance in profile["profile"].balances:
                token_addr = balance.token_contract_address
                token_holders[token_addr].append({
                    "address": address,
                    "amount": balance.amount,
                    "value": balance.value
                })
        
        # ç­›é€‰è¢«>=3ä¸ªåœ°å€æŒæœ‰ä¸”æ€»èµ„äº§>10kçš„ä»£å¸
        common_tokens = {}
        qualified_token_addresses = []  # ç”¨äºåç»­æ‰¹é‡æŸ¥è¯¢ä»£å¸ä¿¡æ¯
        
        for token_addr, holders in token_holders.items():
            if len(holders) >= 3:
                total_value = sum(float(h["value"]) for h in holders if h["value"])
                
                # åªä¿ç•™æ€»èµ„äº§>10kçš„ä»£å¸
                if total_value > 10000:
                    common_tokens[token_addr] = {
                        "holder_count": len(holders),
                        "total_value": total_value,
                        "avg_value": total_value / len(holders),
                        "holders": holders
                    }
                    qualified_token_addresses.append(token_addr)
        
        # åªè·å–ç¬¦åˆæ¡ä»¶çš„ä»£å¸ä¿¡æ¯
        token_info_map = {}
        if qualified_token_addresses:
            token_info_map = self._get_token_info(qualified_token_addresses)
        
        # æ·»åŠ ä»£å¸ä¿¡æ¯åˆ°common_tokens
        for token_addr in common_tokens:
            token_info = token_info_map.get(token_addr, {})
            common_tokens[token_addr].update({
                "symbol": token_info.get('symbol', f"{token_addr[:6]}..."),
                "name": token_info.get('name', f"Unknown Token ({token_addr[:6]}...)"),
                "decimals": token_info.get('decimals', 6)
            })
        
        # æŒ‰æŒæœ‰äººæ•°æ’åº
        sorted_tokens = sorted(common_tokens.items(), 
                             key=lambda x: x[1]["holder_count"], 
                             reverse=True)
        
        return {
            "total_common_tokens": len(common_tokens),
            "top_common_tokens": dict(sorted_tokens[:20]),  # å‰20ä¸ª
            "token_info_map": token_info_map,  # åªåŒ…å«ç¬¦åˆæ¡ä»¶çš„ä»£å¸ä¿¡æ¯
            "summary": {
                "most_held_token": sorted_tokens[0] if sorted_tokens else None,
                "avg_holders_per_token": sum(t["holder_count"] for t in common_tokens.values()) / len(common_tokens) if common_tokens else 0
            }
        }
    
    def _analyze_target_token_ranking(self, holder_profiles: List[Dict], 
                                    target_token: str) -> Dict[str, Any]:
        """åˆ†æç›®æ ‡ä»£å¸åœ¨å„é’±åŒ…ä¸­çš„æ’å"""
        rankings = []
        
        for profile in holder_profiles:
            balances = profile["profile"].balances
            # æŒ‰ä»·å€¼æ’åº
            sorted_balances = sorted(balances, 
                                   key=lambda x: float(x.value) if x.value else 0, 
                                   reverse=True)
            
            # æ‰¾åˆ°ç›®æ ‡ä»£å¸çš„æ’å
            for rank, balance in enumerate(sorted_balances, 1):
                if balance.token_contract_address == target_token:
                    rankings.append(rank)
                    break
        
        if not rankings:
            return {"error": "æ— æ³•è®¡ç®—ç›®æ ‡ä»£å¸æ’å"}
        
        # ç»Ÿè®¡æ’ååˆ†å¸ƒ
        ranking_dist = Counter(rankings)
        
        return {
            "rankings": rankings,
            "median_rank": statistics.median(rankings),
            "avg_rank": sum(rankings) / len(rankings),
            "ranking_distribution": dict(ranking_dist),
            "top_ranked_count": sum(1 for r in rankings if r <= 3),  # å‰3åçš„é’±åŒ…æ•°
            "summary": {
                "best_rank": min(rankings),
                "worst_rank": max(rankings),
                "top_5_percentage": sum(1 for r in rankings if r <= 5) / len(rankings) * 100
            }
        }
    
    def _analyze_clusters(self, holder_profiles: List[Dict]) -> Dict[str, Any]:
        """é›†ç¾¤åˆ†æï¼šè®¡ç®—é›†ç¾¤åˆ†æ•°"""
        # ä¸ºæ¯ä¸ªåœ°å€å»ºç«‹ä»£å¸é›†åˆï¼ˆæ’é™¤ä¸»æµå¸ï¼‰
        address_tokens = {}
        for profile in holder_profiles:
            address = profile["address"]
            tokens = set()
            for balance in profile["profile"].balances:
                if balance.token_contract_address not in self.excluded_tokens:
                    if float(balance.value) > 1:  # ä»·å€¼>1USDçš„ä»£å¸æ‰è®¡å…¥
                        tokens.add(balance.token_contract_address)
            address_tokens[address] = tokens
        
        # è®¡ç®—åœ°å€é—´çš„ç›¸ä¼¼åº¦å’Œé›†ç¾¤
        clusters = []
        addresses = list(address_tokens.keys())
        
        for i in range(len(addresses)):
            for j in range(i + 1, len(addresses)):
                addr1, addr2 = addresses[i], addresses[j]
                tokens1, tokens2 = address_tokens[addr1], address_tokens[addr2]
                
                # è®¡ç®—å…±åŒä»£å¸
                common_tokens = tokens1 & tokens2
                if len(common_tokens) >= 2:  # è‡³å°‘2ä¸ªå…±åŒä»£å¸
                    similarity_score = len(common_tokens)
                    clusters.append({
                        "addresses": [addr1, addr2],
                        "common_tokens": list(common_tokens),
                        "common_token_count": len(common_tokens),
                        "cluster_score": 2 * len(common_tokens)  # 2ä¸ªåœ°å€ * å…±åŒä»£å¸æ•°
                    })
        
        # å°è¯•æ‰¾åˆ°æ›´å¤§çš„é›†ç¾¤ï¼ˆ3ä¸ªæˆ–æ›´å¤šåœ°å€ï¼‰
        large_clusters = self._find_large_clusters(address_tokens)
        
        # æŒ‰é›†ç¾¤åˆ†æ•°æ’åº
        clusters.sort(key=lambda x: x["cluster_score"], reverse=True)
        
        return {
            "total_clusters": len(clusters),
            "large_clusters": large_clusters,
            "top_clusters": clusters[:10],  # å‰10ä¸ªé›†ç¾¤
            "cluster_summary": {
                "max_cluster_score": clusters[0]["cluster_score"] if clusters else 0,
                "avg_cluster_score": sum(c["cluster_score"] for c in clusters) / len(clusters) if clusters else 0,
                "addresses_in_clusters": len(set().union(*[c["addresses"] for c in clusters])) if clusters else 0
            }
        }
    
    def _find_large_clusters(self, address_tokens: Dict[str, set]) -> List[Dict]:
        """å¯»æ‰¾å¤§å‹é›†ç¾¤ï¼ˆ3ä¸ªæˆ–æ›´å¤šåœ°å€çš„å…±åŒæŒä»“ï¼‰"""
        addresses = list(address_tokens.keys())
        large_clusters = []
        
        # ä½¿ç”¨ç®€å•çš„è´ªå¿ƒç®—æ³•å¯»æ‰¾é›†ç¾¤
        for token in set().union(*address_tokens.values()):
            # æ‰¾åˆ°æŒæœ‰è¯¥ä»£å¸çš„æ‰€æœ‰åœ°å€
            holders = [addr for addr, tokens in address_tokens.items() if token in tokens]
            
            if len(holders) >= 3:
                # è®¡ç®—è¿™äº›åœ°å€çš„å…±åŒä»£å¸
                common_tokens = set(address_tokens[holders[0]])
                for addr in holders[1:]:
                    common_tokens &= address_tokens[addr]
                
                if len(common_tokens) >= 2:  # è‡³å°‘2ä¸ªå…±åŒä»£å¸
                    cluster_score = len(holders) * len(common_tokens)
                    large_clusters.append({
                        "addresses": holders,
                        "address_count": len(holders),
                        "common_tokens": list(common_tokens),
                        "common_token_count": len(common_tokens),
                        "cluster_score": cluster_score
                    })
        
        # å»é‡å’Œæ’åº
        unique_clusters = []
        seen_address_sets = set()
        
        for cluster in large_clusters:
            addr_set = frozenset(cluster["addresses"])
            if addr_set not in seen_address_sets:
                seen_address_sets.add(addr_set)
                unique_clusters.append(cluster)
        
        unique_clusters.sort(key=lambda x: x["cluster_score"], reverse=True)
        return unique_clusters[:5]  # è¿”å›å‰5ä¸ªå¤§é›†ç¾¤
    
    def generate_detective_report(self, result: Dict[str, Any], 
                                token_symbol: str = None, 
                                top_holdings_count: int = 20) -> str:
        """
        ç”ŸæˆDETECTIVEé£æ ¼çš„ç¾è§‚åˆ†ææŠ¥å‘Š
        
        Args:
            result: åˆ†æç»“æœå­—å…¸
            token_symbol: ä»£å¸ç¬¦å·ï¼Œå¦‚æœæœªæä¾›ä¼šå°è¯•ä»åœ°å€æ¨æ–­
            top_holdings_count: æ˜¾ç¤ºå‰Nå¤§æŒä»“çš„æ’å
            
        Returns:
            æ ¼å¼åŒ–çš„æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        if "error" in result:
            return f"âŒ åˆ†æå¤±è´¥: {result['error']}"
        
        # è·å–åŸºæœ¬ä¿¡æ¯
        token_address = result['token_address']
        
        # è·å–ç›®æ ‡ä»£å¸çš„ç¬¦å· - ä»å…±åŒæŒä»“ä¿¡æ¯æˆ–ä»£å¸ä¿¡æ¯æ˜ å°„ä¸­è·å–
        if not token_symbol:
            token_info_map = result.get('common_holdings', {}).get('token_info_map', {})
            target_token_info = token_info_map.get(token_address, {})
            token_symbol = target_token_info.get('symbol', token_address[:8] + "...")
        
        analyzed_count = result['analyzed_addresses']
        common_holdings = result['common_holdings']
        top_tokens = common_holdings.get('top_common_tokens', {})
        
        # è®¡ç®—æ€»èµ„äº§å’Œä»£å¸ç§ç±»
        total_value = 0
        token_types = 0
        
        # ä»è¯¦ç»†æŒæœ‰è€…æ•°æ®è®¡ç®—æ€»èµ„äº§
        for holder in result.get('detailed_holders', []):
            if holder.get('profile') and holder['profile'].balances:
                for balance in holder['profile'].balances:
                    try:
                        total_value += float(balance.value)
                    except:
                        pass
        
        token_types = len([info for info in top_tokens.values() if info['holder_count'] >= 3 and info['total_value'] > 10000])
        
        # æ„å»ºæŠ¥å‘Š
        report_lines = []
        report_lines.append(f"ğŸ”¥ {token_symbol}å¤§æˆ·ä¸»è¦æŒä»“ (ğŸ‘¥ æŒ‰æŒæœ‰äººæ•°æ’åº)")
        report_lines.append(f"ğŸ’° æ€»èµ„äº§: ${total_value:,.0f}")
        report_lines.append(f"ğŸ”¢ ä»£å¸ç§ç±»: {token_types}")
        report_lines.append("â”€" * 39)
        
        # æ’åºå¹¶æ˜¾ç¤ºå‰Nä¸ªä»£å¸ï¼ˆåªæ˜¾ç¤ºè¢«â‰¥3äººæŒæœ‰çš„ï¼‰
        sorted_tokens = sorted(top_tokens.items(), 
                             key=lambda x: x[1]['holder_count'], 
                             reverse=True)
        
        # è¿‡æ»¤æ‰æŒæœ‰äººæ•°å°‘äº3æˆ–æ€»èµ„äº§å°äº10kçš„ä»£å¸
        filtered_tokens = [(addr, info) for addr, info in sorted_tokens if info['holder_count'] >= 3 and info['total_value'] > 10000]
        
        for i, (token_addr, token_info) in enumerate(filtered_tokens[:top_holdings_count], 1):
            holder_count = token_info['holder_count']
            total_token_value = token_info['total_value']
            
            # ä»ä»£å¸ä¿¡æ¯ä¸­è·å–ç¬¦å·å’Œåç§°
            symbol = token_info.get('symbol', f"{token_addr[:6]}...")
            name = token_info.get('name', '')
            
            # æ„å»ºæ˜¾ç¤ºåç§°
            if name and name != symbol:
                display_name = f"{symbol} ({name})"
            else:
                display_name = symbol
            
            # ç”Ÿæˆ gmgn é“¾æ¥
            gmgn_url = f" (https://gmgn.ai/sol/token/{token_addr})"
            
            # æ ¼å¼åŒ–é‡‘é¢
            if total_token_value >= 1000000:
                value_str = f"${total_token_value/1000000:.1f}M"
            elif total_token_value >= 1000:
                value_str = f"${total_token_value/1000:.1f}K"
            else:
                value_str = f"${total_token_value:.0f}"
            
            report_lines.append(f"{i:2d}. {display_name}{gmgn_url} ({holder_count}äºº) {value_str}")
        
        # æ·»åŠ åˆ†æç»Ÿè®¡
        report_lines.append("")
        report_lines.append("ğŸ‘¥ åˆ†æäººç±»åœ°å€:")
        
        # æ‰¾åˆ°ç›®æ ‡ä»£å¸çš„æŒæœ‰äººæ•° - åœ¨å·²åˆ†æçš„åœ°å€ä¸­
        target_token_holders = 0
        for token_addr, token_info in top_tokens.items():
            if token_addr == result['token_address']:
                target_token_holders = token_info['holder_count']
                break
        
        # å¦‚æœåœ¨å…±åŒæŒä»“ä¸­æ²¡æ‰¾åˆ°ç›®æ ‡ä»£å¸ï¼Œè¯´æ˜æŒæœ‰äººæ•°å°‘äº3ä¸ª
        if target_token_holders == 0:
            # ä»è¯¦ç»†æŒæœ‰è€…æ•°æ®ä¸­ç»Ÿè®¡å®é™…æŒæœ‰ç›®æ ‡ä»£å¸çš„äººæ•°
            target_token_holders = 0
            for holder in result.get('detailed_holders', []):
                if holder.get('target_token_balance') and float(holder['target_token_balance']) > 0:
                    target_token_holders += 1
            
            # å¦‚æœè¿˜æ˜¯0ï¼Œä½¿ç”¨å·²åˆ†æåœ°å€æ•°ä½œä¸ºå¤‡é€‰
            if target_token_holders == 0:
                target_token_holders = analyzed_count
        
        total_human_holders = result.get('human_holders', analyzed_count)
        
        report_lines.append(f"ğŸ¯ å®é™…æŒæœ‰ {token_symbol.replace('...', '')}: {target_token_holders} äºº (å…±åˆ†æ {analyzed_count} ä¸ªåœ°å€)")
        report_lines.append(f"ğŸ“ˆ ç»Ÿè®¡èŒƒå›´: åˆ†ææ¯ä¸ªåœ°å€çš„å®Œæ•´æŒä»“ï¼Œæ˜¾ç¤ºè¢«â‰¥3äººæŒæœ‰çš„ä»£å¸")
        report_lines.append(f"ğŸ“Š æ€»ä½“æƒ…å†µ: {total_human_holders} ä¸ªçœŸäººåœ°å€ä¸­çš„ {analyzed_count} ä¸ªå·²å®Œæˆåˆ†æ")
        
        # æ˜¾ç¤ºç›®æ ‡ä»£å¸ä¸åœ¨å‰20èµ„äº§ä¸­çš„åœ°å€
        not_in_top20_holders = []
        for addr, data in result.get('detailed_analysis', {}).items():
            if not data.get('target_in_top20', True):  # é»˜è®¤Trueï¼Œåªæœ‰æ˜ç¡®ä¸ºFalseæ‰ç®—
                not_in_top20_holders.append(addr)
        
        if not_in_top20_holders:
            report_lines.append(f"\nğŸ“‰ ç›®æ ‡ä»£å¸ä¸åœ¨å‰20èµ„äº§ä¸­çš„åœ°å€ ({len(not_in_top20_holders)} ä¸ª):")
            for i, addr in enumerate(not_in_top20_holders, 1):
                report_lines.append(f"   {i}. {addr[:6]}...{addr[-4:]}")
            report_lines.append(f"   æ³¨: è¿™äº›åœ°å€æŒæœ‰ç›®æ ‡ä»£å¸ï¼Œä½†é‡‘é¢è¾ƒå°ï¼Œæœªè¿›å…¥ä¸ªäººå‰20èµ„äº§æ’è¡Œ")
        
        # æ·»åŠ éäººç±»åœ°å€ä¿¡æ¯
        all_holders = result.get('all_holders', [])
        non_human_holders = [h for h in all_holders if h.get('tag') != 'human']
        
        if non_human_holders:
            report_lines.append("")
            report_lines.append("ğŸ¦ éäººç±»åœ°å€:")
            
            # æŒ‰ç±»å‹åˆ†ç»„
            pools = [h for h in non_human_holders if h.get('tag') == 'pool']
            exchanges = [h for h in non_human_holders if h.get('tag') == 'exchange']
            contracts = [h for h in non_human_holders if h.get('tag') == 'contract']
            
            # æ˜¾ç¤ºæµåŠ¨æ€§æ± 
            if pools:
                report_lines.append(f"ğŸ’§ æµåŠ¨æ€§æ±  ({len(pools)}ä¸ª):")
                for pool in pools:
                    balance = float(pool.get('balance', 0))
                    value = float(pool.get('value_usd', 0))
                    if balance >= 1000000:
                        balance_str = f"{balance/1000000:.1f}M"
                    elif balance >= 1000:
                        balance_str = f"{balance/1000:.1f}K"
                    else:
                        balance_str = f"{balance:.0f}"
                    
                    if value >= 1000:
                        value_str = f"${value/1000:.1f}K"
                    else:
                        value_str = f"${value:.0f}"
                    
                    address_short = pool['address'][:6] + "..." + pool['address'][-4:]
                    report_lines.append(f"   â€¢ {address_short}: {balance_str} ({value_str})")
            
            # æ˜¾ç¤ºäº¤æ˜“æ‰€
            if exchanges:
                report_lines.append(f"ğŸ›ï¸ äº¤æ˜“æ‰€ ({len(exchanges)}ä¸ª):")
                for exchange in exchanges:
                    balance = float(exchange.get('balance', 0))
                    value = float(exchange.get('value_usd', 0))
                    if balance >= 1000000:
                        balance_str = f"{balance/1000000:.1f}M"
                    elif balance >= 1000:
                        balance_str = f"{balance/1000:.1f}K"
                    else:
                        balance_str = f"{balance:.0f}"
                    
                    if value >= 1000:
                        value_str = f"${value/1000:.1f}K"
                    else:
                        value_str = f"${value:.0f}"
                    
                    address_short = exchange['address'][:6] + "..." + exchange['address'][-4:]
                    report_lines.append(f"   â€¢ {address_short}: {balance_str} ({value_str})")
            
            # æ˜¾ç¤ºåˆçº¦
            if contracts:
                report_lines.append(f"ğŸ“œ åˆçº¦ ({len(contracts)}ä¸ª):")
                for contract in contracts:
                    balance = float(contract.get('balance', 0))
                    value = float(contract.get('value_usd', 0))
                    if balance >= 1000000:
                        balance_str = f"{balance/1000000:.1f}M"
                    elif balance >= 1000:
                        balance_str = f"{balance/1000:.1f}K"
                    else:
                        balance_str = f"{balance:.0f}"
                    
                    if value >= 1000:
                        value_str = f"${value/1000:.1f}K"
                    else:
                        value_str = f"${value:.0f}"
                    
                    address_short = contract['address'][:6] + "..." + contract['address'][-4:]
                    report_lines.append(f"   â€¢ {address_short}: {balance_str} ({value_str})")
        
        return "\n".join(report_lines)
    
    def _get_token_symbol(self, token_address: str) -> str:
        """è·å–ä»£å¸ç¬¦å·çš„ç®€åŒ–æ˜¾ç¤ºåç§°"""
        # å·²çŸ¥ä»£å¸åœ°å€æ˜ å°„
        known_tokens = {
            "EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v": "USDC",
            "Es9vMFrzaCERmJfrF4H2FYD4KCoNkY11McCe8BenwNYB": "USDT", 
            "4k3Dyjzvzp8eMZWUXbBCjEvwSkkk59S5iCNLY3QrkX6R": "RAY",
            "SRMuApVNdxXokk5GT7XD5cUUgXMBCoAz2LHeuAoKWRt": "SRM",
            "So11111111111111111111111111111111111111112": "SOL"
        }
        
        if token_address in known_tokens:
            return known_tokens[token_address]
        else:
            # å¯¹äºæœªçŸ¥ä»£å¸ï¼Œè¿”å›åœ°å€çš„å‰å‡ ä½
            return token_address[:6] + "..."
    
    def save_analysis_result(self, result: Dict[str, Any], 
                           filename: Optional[str] = None) -> str:
        """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            token_short = result["token_address"][:8]
            filename = f"token_analysis_{token_short}_{timestamp}.yaml"
        
        filepath = os.path.join("storage", filename)
        
        # ç¡®ä¿storageç›®å½•å­˜åœ¨
        os.makedirs("storage", exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            yaml.dump(result, f, default_flow_style=False, 
                     allow_unicode=True, sort_keys=False)
        
        print(f"ğŸ“ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath


def main():
    """æµ‹è¯•å‡½æ•° - å¯åœ¨è¿™é‡Œä¿®æ”¹æ€§èƒ½æ¨¡å¼"""
    
    # ğŸ”§ åœ¨è¿™é‡Œä¿®æ”¹æ€§èƒ½æ¨¡å¼:
    # 'conservative' = ä¿å®ˆæ¨¡å¼ (1.2 åœ°å€/ç§’) - æœ€ç¨³å®š
    # 'balanced'     = å¹³è¡¡æ¨¡å¼ (2.6 åœ°å€/ç§’) - æ¨èä½¿ç”¨  
    # 'high_speed'   = é«˜é€Ÿæ¨¡å¼ (3.4 åœ°å€/ç§’) - æé™é€Ÿåº¦
    # 'lightweight'  = è½»é‡æ¨¡å¼ (2.0 åœ°å€/ç§’) - ç½‘ç»œä¸ä½³æ—¶ä½¿ç”¨
    
    PERFORMANCE_MODE = 'high_speed'  # ğŸ‘ˆ åœ¨è¿™é‡Œä¿®æ”¹æ¨¡å¼
    
    print(f"ğŸ”§ ä½¿ç”¨æ€§èƒ½æ¨¡å¼: {PERFORMANCE_MODE}")
    analyzer = TokenHolderAnalyzer(performance_mode=PERFORMANCE_MODE)
    
    # è®¾ç½®è®¤è¯ä¿¡æ¯ï¼ˆéœ€è¦æ›¿æ¢ä¸ºçœŸå®æ•°æ®ï¼‰
    analyzer.set_auth(
        cookie="devId=01980a38-038a-44d9-8da3-a8276bbcb5b9; ok_site_info===QfxojI5RXa05WZiwiIMFkQPx0Rfh1SPJiOiUGZvNmIsICUKJiOi42bpdWZyJye; locale=en_US; ok_prefer_currency=0%7C1%7Cfalse%7CUSD%7C2%7C%24%7C1%7C1%7CUSD; ok_prefer_udColor=0; ok_prefer_udTimeZone=0; fingerprint_id=01980a38-038a-44d9-8da3-a8276bbcb5b9; first_ref=https%3A%2F%2Fweb3.okx.com%2Ftoken%2Fsolana%2FFbGsCHv8qPvUdmomVAiG72ET5D5kgBJgGoxxfMZipump; ok_global={%22g_t%22:2}; _gcl_au=1.1.1005719754.1755091396; connectedWallet=1; _gid=GA1.2.950489538.1757092345; mse=nf=8|se=0; __cf_bm=KlSlz4ToD2eBrbV2YMpvOTgZSH9aJx8ZbSpNehX__70-1757227578-1.0.1.1-CVB_X0rNpOfUw.n3YJgAepDber7b9fzyAdFONBE5xbbJ9uokVrU0D0ZnKpCgKqWRX9MNMHAODFPNpxZDZYUw1XLYVw6RbsONqf7J5SbrKAc; ok-exp-time=1757227583876; okg.currentMedia=md; tmx_session_id=g42rqe6lkgv_1757227586034; connected=1; fp_s=0; traceId=2130772279702400005; _gat_UA-35324627-3=1; _ga=GA1.1.2083537763.1750302376; _ga_G0EKWWQGTZ=GS2.1.s1757227595$o127$g1$t1757227972$j58$l0$h0; ok-ses-id=ic8FZdwDJ9iztku9zy3wjshp7WSUVWnCq6wpmGltOew4BJU1wkFkGYHyg2jS3JIKpZCB7dnA0g1BCrndYsGLeFEXC9fKYuWwNU4qCZlHwpNQI42XTE4EYPY03Z1p2MaR; _monitor_extras={\"deviceId\":\"KmpeI8VVHan-2zL3_DbOJB\",\"eventId\":6313,\"sequenceNumber\":6313}",
        fp_token="eyJraWQiOiIxNjgzMzgiLCJhbGciOiJFUzI1NiJ9.eyJpYXQiOjE3NTcyMjc1ODksImVmcCI6Ikw1bHcvc0JPNENzV040MXB4MVlTLzRUSjBQaDViZlg0QjhiMTQrTGlHK21NdEt6WWx4TWpFdi9yK0o2MXlCcnIiLCJkaWQiOiIwMTk4MGEzOC0wMzhhLTQ0ZDktOGRhMy1hODI3NmJiY2I1YjkiLCJjcGsiOiJNRmt3RXdZSEtvWkl6ajBDQVFZSUtvWkl6ajBEQVFjRFFnQUVGOGE4RnFDNElLWDJxSHFaOHhaamJnd3BiMDloU2VCSWdxSkdjZ1FEWng0SEp2Z1lIN0g3NE5QblZsRHFWWWNUR0VBWm41aUw4bWdEQTVKbjY5SHJ5Zz09In0.Z1TlLWz0sQ3TvPxo6czcnmZjw1oQ20rvscyTT0CNCx3_e9zPa2WJrfppdAlJ5GbrqqEwyfI2upOd-fy2UGFnSg",
        verify_sign="z0wcDnWum9Gxbbxbq+G6gvmUd7xATTa7V+XX5HvXEe4=",
        verify_token="ac90bf8e-b5fc-4643-a441-2d7b7eb08634",
        dev_id="01980a38-038a-44d9-8da3-a8276bbcb5b9",
        site_info="==QfxojI5RXa05WZiwiIMFkQPx0Rfh1SPJiOiUGZvNmIsICUKJiOi42bpdWZyJye"
    )
    
    # åˆ†æç¤ºä¾‹ä»£å¸
    token_address = "9X45NjtGbGo9zdCFmMyqZyNzC6Wa67KFbfvGc8nubonk"
    
    print("ğŸš€ å¼€å§‹ä»£å¸æŒæœ‰è€…åˆ†æ...")
    result = analyzer.analyze_token_holders(token_address)
    
    if "error" in result:
        print(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
        return
    
    # ä¿å­˜ç»“æœ
    filepath = analyzer.save_analysis_result(result)
    
    # ç”Ÿæˆå¹¶è¾“å‡º
    print("="*60)
    detective_report = analyzer.generate_detective_report(result, "DETECTIVE")
    print(detective_report)
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {filepath}")


if __name__ == "__main__":
    main()

