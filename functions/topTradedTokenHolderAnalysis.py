#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çƒ­é—¨äº¤æ˜“ä»£å¸æŒæœ‰è€…åˆ†ææ¨¡å—
Top Traded Token Holder Analysis Module

è¾“å…¥: ä½¿ç”¨Jupiteré¢„è®¾é…ç½®çˆ¬å–çƒ­é—¨äº¤æ˜“ä»£å¸
è¾“å‡º: æŒ‰ç…§tokenHolderAnalysisé€ä¸ªè¾“å‡ºç¬¦åˆæ¡ä»¶çš„ä»£å¸æŠ¥å‘Š
ç­›é€‰æ¡ä»¶: ç›®æ ‡ä»£å¸å¤§æˆ·è‡³å°‘5ä¸ªåœ°å€æŒæœ‰åŒä¸€ä¸ªä»£å¸(é™¤äº†ç›®æ ‡ä»£å¸, ç¨³å®šå¸, SOL), è¯¥ä»£å¸æŒæœ‰è€…çš„æ€»ä»·å€¼è¶…è¿‡10ä¸‡ç¾å…ƒ

Input: Use Jupiter preset configuration to crawl top traded tokens
Output: Output qualified token reports one by one according to tokenHolderAnalysis
Filter condition: Target token holders must have at least 5 addresses holding the same token (excluding target token, stablecoins, SOL), with total value exceeding $100,000
"""

import sys
import os
import time
from datetime import datetime
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from crawlers.jupiter.topTradedTokens import JupiterTopTradedCrawler
from functions.tokenHolderAnalysis import TokenHolderAnalyzer
from functions.logger import CrawlerLogger, get_logger
from settings.config_manager import ConfigManager


class TopTradedTokenHolderAnalyzer:
    """çƒ­é—¨äº¤æ˜“ä»£å¸æŒæœ‰è€…åˆ†æå™¨"""
    
    def __init__(self, performance_mode: str = 'high_speed'):
        """åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            performance_mode: æ€§èƒ½æ¨¡å¼ ('conservative', 'balanced', 'high_speed', 'lightweight')
        """
        self.performance_mode = performance_mode
        self.config = ConfigManager()
        self.jupiter_crawler = JupiterTopTradedCrawler()
        self.holder_analyzer = TokenHolderAnalyzer(performance_mode=performance_mode)
        
        # åˆå§‹åŒ–æ—¥å¿—å™¨
        self.logger = CrawlerLogger("TopTradedTokenAnalyzer")
        
        # æ›´ä¸¥æ ¼çš„ç­›é€‰æ¡ä»¶
        self.min_holders = 5  # è‡³å°‘5ä¸ªåœ°å€æŒæœ‰
        self.min_total_value = 300000  # æ€»ä»·å€¼è¶…è¿‡30ä¸‡ç¾å…ƒ
        
        # ä¸»æµç¨³å®šå¸å’ŒSOLåœ°å€ï¼Œç­›é€‰æ—¶æ’é™¤
        self.excluded_tokens = {
            "So11111111111111111111111111111111111111112",  # SOL å®Œæ•´åœ°å€
            "EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v",  # USDC
            "Es9vMFrzaCERmJfrF4H2FYD4KCoNkY11McCe8BenwNYB",  # USDT
        }
        
    def set_auth(self, cookie: str, fp_token: str, verify_sign: str, 
                 verify_token: str, dev_id: str, site_info: str):
        """è®¾ç½®OKXè®¤è¯ä¿¡æ¯"""
        self.holder_analyzer.set_auth(cookie, fp_token, verify_sign, 
                                    verify_token, dev_id, site_info)
    
    def analyze_top_traded_tokens(self, preset_name: str = 'lowCapGem_24h',
                                max_tokens: int = 20,
                                delay_between_tokens: float = 2.0,
                                progress_callback=None,
                                qualified_callback=None) -> List[Dict[str, Any]]:
        """
        åˆ†æçƒ­é—¨äº¤æ˜“ä»£å¸çš„æŒæœ‰è€…
        
        Args:
            preset_name: Jupiteré¢„è®¾åç§°, é»˜è®¤ä¸º'lowCapGem_24h'
            max_tokens: æœ€å¤§åˆ†æä»£å¸æ•°é‡
            delay_between_tokens: ä»£å¸åˆ†æé—´éš”æ—¶é—´(ç§’)
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            qualified_callback: å‘ç°ç¬¦åˆæ¡ä»¶ä»£å¸æ—¶çš„å›è°ƒå‡½æ•°
            
        Returns:
            ç¬¦åˆæ¡ä»¶çš„åˆ†æç»“æœåˆ—è¡¨
        """
        # è®°å½•ä»»åŠ¡å¼€å§‹
        task_desc = f"çƒ­é—¨äº¤æ˜“ä»£å¸æŒæœ‰è€…åˆ†æ - é¢„è®¾:{preset_name}, æ€§èƒ½æ¨¡å¼:{self.performance_mode}"
        self.logger.log_start(task_desc)
        
        self.logger.info(f"ğŸ“‹ ç­›é€‰æ¡ä»¶: è‡³å°‘{self.min_holders}ä¸ªåœ°å€æŒæœ‰, æ€»ä»·å€¼>${self.min_total_value:,}(æ’é™¤SOL, USDC, USDTç­‰ä¸»æµå¸)")
        
        # 1. è·å–çƒ­é—¨äº¤æ˜“ä»£å¸
        try:
            self.logger.info("ğŸ”„ æ­£åœ¨è·å–çƒ­é—¨äº¤æ˜“ä»£å¸...")
            tokens = self.jupiter_crawler.crawl_with_preset(preset_name)
            self.logger.log_success("è·å–çƒ­é—¨äº¤æ˜“ä»£å¸", f"è·å–åˆ° {len(tokens)} ä¸ªä»£å¸")
        except Exception as e:
            self.logger.log_error("è·å–çƒ­é—¨äº¤æ˜“ä»£å¸", str(e))
            return []
        
        if not tokens:
            self.logger.log_error("è·å–çƒ­é—¨äº¤æ˜“ä»£å¸", "æœªè·å–åˆ°ä»»ä½•ä»£å¸æ•°æ®")
            return []
        
        # é™åˆ¶åˆ†ææ•°é‡
        tokens_to_analyze = tokens[:max_tokens]
        self.logger.info(f"ğŸ“ å°†åˆ†æå‰ {len(tokens_to_analyze)} ä¸ªä»£å¸")
        
        # 2. é€ä¸ªåˆ†æä»£å¸æŒæœ‰è€…
        qualified_results = []
        
        for i, token in enumerate(tokens_to_analyze, 1):
            try:
                # æ›´æ–°è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(i, len(tokens_to_analyze))
                
                # è®°å½•å½“å‰å¤„ç†è¿›åº¦
                self.logger.log_progress(
                    i, len(tokens_to_analyze), 
                    f"{token.symbol} ({token.contract_address[:8]}...)",
                    "åˆ†æä¸­"
                )
                
                # æ˜¾ç¤ºä»£å¸åŸºæœ¬ä¿¡æ¯
                if hasattr(token, 'market_cap') and token.market_cap:
                    self.logger.info(f"ğŸ’° å¸‚å€¼: ${token.market_cap:,.0f}")
                
                # åˆ†ææŒæœ‰è€…
                analysis_result = self.holder_analyzer.analyze_token_holders(
                    token.contract_address,
                    chain_id="501"
                )
                
                if "error" in analysis_result:
                    self.logger.log_error(f"åˆ†æä»£å¸ {token.symbol}", analysis_result['error'])
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ç¬¦åˆæ¡ä»¶
                if self._check_qualification(analysis_result, token):
                    self.logger.log_qualified_found(
                        f"{token.symbol} ({token.contract_address[:8]}...)",
                        f"ç¬¦åˆæŒæœ‰è€…æ¡ä»¶ - å¤§æˆ·å…±åŒæŒä»“åˆ†æ"
                    )
                    
                    # æ·»åŠ ä»£å¸åŸºæœ¬ä¿¡æ¯
                    analysis_result['token_info'] = {
                        'symbol': token.symbol,
                        'name': token.name,
                        'contract_address': token.contract_address,
                        'market_cap': getattr(token, 'market_cap', None),
                        'volume_24h': getattr(token, 'volume_24h', None),
                        'price': getattr(token, 'price', None)
                    }
                    
                    qualified_results.append(analysis_result)
                    
                    # ğŸš€ ç«‹å³é€šè¿‡å›è°ƒå‘é€åˆ°ç¾¤ç»„
                    if qualified_callback:
                        try:
                            self.logger.info(f"ğŸ¯ å‘ç°ç¬¦åˆæ¡ä»¶çš„ä»£å¸: {token.symbol}, ç«‹å³å‘é€åˆ°ç¾¤ç»„")
                            qualified_callback(analysis_result)
                            self.logger.info(f"âœ… å·²ç«‹å³è¾“å‡ºç¬¦åˆæ¡ä»¶çš„ä»£å¸: {token.symbol}")
                        except Exception as e:
                            self.logger.error(f"âŒ ç«‹å³å‘é€ä»£å¸ {token.symbol} å¤±è´¥: {e}")
                    
                    # ç”Ÿæˆå¹¶è¾“å‡ºæŠ¥å‘Šåˆ°æ—¥å¿—
                    self._output_token_report(analysis_result, token)
                    
                    self.logger.log_success(f"åˆ†æä»£å¸ {token.symbol}", "ç¬¦åˆæ¡ä»¶, å·²æ·»åŠ åˆ°ç»“æœ")
                    
                else:
                    self.logger.info(f"âŒ {token.symbol} ä¸ç¬¦åˆæ¡ä»¶")
                
                # å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                if i < len(tokens_to_analyze):
                    self.logger.debug(f"â±ï¸ ç­‰å¾… {delay_between_tokens} ç§’...")
                    time.sleep(delay_between_tokens)
                    
            except Exception as e:
                self.logger.log_error(f"åˆ†æä»£å¸ {token.symbol}", str(e))
                continue
        
        # 3. è¾“å‡ºæ€»ç»“
        self.logger.log_completion(task_desc)
        self._output_summary(qualified_results, len(tokens_to_analyze))
        
        return qualified_results
    
    def _check_qualification(self, analysis_result: Dict[str, Any], token) -> bool:
        """
        æ£€æŸ¥åˆ†æç»“æœæ˜¯å¦ç¬¦åˆç­›é€‰æ¡ä»¶
        
        Args:
            analysis_result: åˆ†æç»“æœ
            token: ä»£å¸ä¿¡æ¯
            
        Returns:
            æ˜¯å¦ç¬¦åˆæ¡ä»¶
        """
        common_holdings = analysis_result.get('common_holdings', {})
        top_tokens = common_holdings.get('top_common_tokens', {})
        token_info_map = common_holdings.get('token_info_map', {})
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»£å¸æ»¡è¶³æ¡ä»¶(é™¤äº†ç›®æ ‡ä»£å¸æœ¬èº«)
        target_address = analysis_result['token_address']
        
        for token_addr, token_info in top_tokens.items():
            # è·³è¿‡ç›®æ ‡ä»£å¸æœ¬èº«
            if token_addr == target_address:
                continue
            
            # è·³è¿‡ä¸»æµç¨³å®šå¸å’ŒSOL
            if token_addr in self.excluded_tokens:
                continue
            
            # æ™ºèƒ½è¯†åˆ«SOL: æ£€æŸ¥æ˜¯å¦ä¸ºSOLçš„ç‰¹æ®Šæ ¼å¼
            if self._is_sol_token(token_addr, token_info_map):
                self.logger.debug(f"è·³è¿‡SOLä»£å¸: {token_info.get('symbol', 'Unknown')} (è¯†åˆ«ä¸ºSOL)")
                continue
                
            holder_count = token_info['holder_count']
            total_value = token_info['total_value']
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ¡ä»¶: è‡³å°‘5ä¸ªåœ°å€æŒæœ‰, æ€»ä»·å€¼è¶…è¿‡50ä¸‡ç¾å…ƒ
            if holder_count >= self.min_holders and total_value >= self.min_total_value:
                self.logger.info(f"ğŸ“Š ç¬¦åˆæ¡ä»¶çš„ä»£å¸: {token_info.get('symbol', 'Unknown')} ({holder_count}äººæŒæœ‰, ${total_value:,.0f})")
                return True
        
        self.logger.debug(f"æ— ç¬¦åˆæ¡ä»¶çš„å…±åŒæŒä»“ (éœ€è¦: >={self.min_holders}äººæŒæœ‰ ä¸” >=${self.min_total_value:,}, æ’é™¤ä¸»æµå¸)")
        return False
    
    def _is_sol_token(self, token_addr: str, token_info_map: Dict[str, Any]) -> bool:
        """
        æ™ºèƒ½è¯†åˆ«æ˜¯å¦ä¸ºSOLä»£å¸
        
        Args:
            token_addr: ä»£å¸åœ°å€/é”®å
            token_info_map: ä»£å¸ä¿¡æ¯æ˜ å°„
            
        Returns:
            æ˜¯å¦ä¸ºSOLä»£å¸
        """
        # æ£€æŸ¥æ˜¯å¦ä¸ºå®Œæ•´çš„SOLåœ°å€
        if token_addr == "So11111111111111111111111111111111111111112":
            return True
        
        # æ£€æŸ¥token_info_mapä¸­çš„è¯¦ç»†ä¿¡æ¯
        token_info = token_info_map.get(token_addr, {})
        if token_info:
            symbol = token_info.get('symbol', '').upper()
            name = token_info.get('name', '').upper()
            
            # è¯†åˆ«SOLçš„ç‰¹å¾: symbolåŒ…å«"SOL...SOL", nameåŒ…å«"UNKNOWN TOKEN (SOL"
            if 'SOL...SOL' in symbol or 'UNKNOWN TOKEN (SOL' in name:
                return True
        
        return False
    
    def _output_token_report(self, analysis_result: Dict[str, Any], token) -> None:
        """è¾“å‡ºå•ä¸ªä»£å¸çš„åˆ†ææŠ¥å‘Š"""
        self.logger.info("="*60)
        self.logger.info(f"ğŸ¯ {token.symbol} æŒæœ‰è€…åˆ†ææŠ¥å‘Š")
        self.logger.info("="*60)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = self.holder_analyzer.generate_detective_report(
            analysis_result, 
            token.symbol,
            top_holdings_count=15  # æ˜¾ç¤ºå‰15ä¸ªæŒä»“
        )
        
        # å°†æŠ¥å‘ŠæŒ‰è¡Œåˆ†å‰²å¹¶é€è¡Œè®°å½•æ—¥å¿—
        for line in report.split('\n'):
            if line.strip():  # è·³è¿‡ç©ºè¡Œ
                self.logger.info(line)
        
        self.logger.info("="*60)
        
        # ä¿å­˜åˆ†æç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"toptraded_{token.symbol}_{timestamp}.yaml"
        self.holder_analyzer.save_analysis_result(analysis_result, filename)
        self.logger.info(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜è‡³: {filename}")
    
    def _output_summary(self, qualified_results: List[Dict[str, Any]], total_analyzed: int) -> None:
        """è¾“å‡ºåˆ†ææ€»ç»“"""
        self.logger.info("ğŸ‰"*20)
        self.logger.info("ğŸ“Š åˆ†ææ€»ç»“")
        self.logger.info("ğŸ‰"*20)
        self.logger.info(f"âœ… ç¬¦åˆæ¡ä»¶çš„ä»£å¸: {len(qualified_results)}/{total_analyzed}")
        self.logger.info(f"ğŸ“‹ ç­›é€‰æ ‡å‡†: è‡³å°‘{self.min_holders}ä¸ªåœ°å€æŒæœ‰åŒä¸€ä»£å¸, æ€»ä»·å€¼>=${self.min_total_value:,}(æ’é™¤SOL, USDC, USDTç­‰ä¸»æµå¸)")
        
        if qualified_results:
            self.logger.info("")
            self.logger.info("ğŸ† ç¬¦åˆæ¡ä»¶çš„ä»£å¸åˆ—è¡¨:")
            for i, result in enumerate(qualified_results, 1):
                token_info = result.get('token_info', {})
                symbol = token_info.get('symbol', 'Unknown')
                contract = token_info.get('contract_address', '')
                self.logger.info(f"  {i}. {symbol} ({contract[:8]}...)")
        else:
            self.logger.info("")
            self.logger.info("âŒ æœ¬æ¬¡åˆ†æä¸­æ²¡æœ‰ä»£å¸ç¬¦åˆæ¡ä»¶")
        
        self.logger.info("ğŸ‰"*20)


def main():
    """ä¸»å‡½æ•°"""
    # ğŸ”§ é…ç½®å‚æ•°
    PRESET_NAME = 'lowCapGem_24h'  # Jupiteré¢„è®¾åç§°
    PERFORMANCE_MODE = 'high_speed'   # æ€§èƒ½æ¨¡å¼
    MAX_TOKENS = 1000                 # æœ€å¤§åˆ†æä»£å¸æ•°
    DELAY_BETWEEN_TOKENS = 1.0      # ä»£å¸é—´å»¶è¿Ÿ(ç§’)
    
    # åˆå§‹åŒ–ä¸»æ—¥å¿—å™¨
    main_logger = get_logger("TopTradedAnalysis.Main")
    
    main_logger.info("ğŸ”§ é…ç½®å‚æ•°:")
    main_logger.info(f"   é¢„è®¾åç§°: {PRESET_NAME}")
    main_logger.info(f"   æ€§èƒ½æ¨¡å¼: {PERFORMANCE_MODE}")
    main_logger.info(f"   æœ€å¤§åˆ†ææ•°: {MAX_TOKENS}")
    main_logger.info(f"   é—´éš”æ—¶é—´: {DELAY_BETWEEN_TOKENS}ç§’")
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = TopTradedTokenHolderAnalyzer(performance_mode=PERFORMANCE_MODE)
    
    # è®¾ç½®OKXè®¤è¯ä¿¡æ¯(éœ€è¦æ›¿æ¢ä¸ºçœŸå®æ•°æ®)
    analyzer.set_auth(
        cookie="devId=01980a38-038a-44d9-8da3-a8276bbcb5b9; ok_site_info===QfxojI5RXa05WZiwiIMFkQPx0Rfh1SPJiOi42bpdWZyJye; locale=en_US; ok_prefer_currency=0%7C1%7Cfalse%7CUSD%7C2%7C%24%7C1%7C1%7CUSD; ok_prefer_udColor=0; ok_prefer_udTimeZone=0; fingerprint_id=01980a38-038a-44d9-8da3-a8276bbcb5b9; first_ref=https%3A%2F%2Fweb3.okx.com%2Ftoken%2Fsolana%2FFbGsCHv8qPvUdmomVAiG72ET5D5kgBJgGoxxfMZipump; ok_global={%22g_t%22:2}; _gcl_au=1.1.1005719754.1755091396; connectedWallet=1; _gid=GA1.2.950489538.1757092345; mse=nf=8|se=0; __cf_bm=KlSlz4ToD2eBrbV2YMpvOTgZSH9aJx8ZbSpNehX__70-1757227578-1.0.1.1-CVB_X0rNpOfUw.n3YJgAepDber7b9fzyAdFONBE5xbbJ9uokVrU0D0ZnKpCgKqWRX9MNMHAODFPNpxZDZYUw1XLYVw6RbsONqf7J5SbrKAc; ok-exp-time=1757227583876; okg.currentMedia=md; tmx_session_id=g42rqe6lkgv_1757227586034; connected=1; fp_s=0; traceId=2130772279702400005; _gat_UA-35324627-3=1; _ga=GA1.1.2083537763.1750302376; _ga_G0EKWWQGTZ=GS2.1.s1757227595$o127$g1$t1757227972$j58$l0$h0; ok-ses-id=ic8FZdwDJ9iztku9zy3wjshp7WSUVWnCq6wpmGltOew4BJU1wkFkGYHyg2jS3JIKpZCB7dnA0g1BCrndYsGLeFEXC9fKYuWwNU4qCZlHwpNQI42XTE4EYPY03Z1p2MaR; _monitor_extras={\"deviceId\":\"KmpeI8VVHan-2zL3_DbOJB\",\"eventId\":6313,\"sequenceNumber\":6313}",
        fp_token="eyJraWQiOiIxNjgzMzgiLCJhbGciOiJFUzI1NiJ9.eyJpYXQiOjE3NTcyMjc1ODksImVmcCI6Ikw1bHcvc0JPNENzV040MXB4MVlTLzRUSjBQaDViZlg0QjhiMTQrTGlHK21NdEt6WWx4TWpFdi9yK0o2MXlCcnIiLCJkaWQiOiIwMTk4MGEzOC0wMzhhLTQ0ZDktOGRhMy1hODI3NmJiY2I1YjkiLCJjcGsiOiJNRmt3RXdZSEtvWkl6ajBDQVFZSUtvWkl6ajBEQVFjRFFnQUVGOGE4RnFDNElLWDJxSHFaOHhaamJnd3BiMDloU2VCSWdxSkdjZ1FEWng0SEp2Z1lIN0g3NE5QblZsRHFWWWNUR0VBWm41aUw4bWdEQTVKbjY5SHJ5Zz09In0.Z1TlLWz0sQ3TvPxo6czcnmZjw1oQ20rvscyTT0CNCx3_e9zPa2WJrfppdAlJ5GbrqqEwyfI2upOd-fy2UGFnSg",
        verify_sign="z0wcDnWum9Gxbbxbq+G6gvmUd7xATTa7V+XX5HvXEe4=",
        verify_token="ac90bf8e-b5fc-4643-a441-2d7b7eb08634",
        dev_id="01980a38-038a-44d9-8da3-a8276bbcb5b9",
        site_info="==QfxojI5RXa05WZiwiIMFkQPx0Rfh1SPJiOi42bpdWZyJye"
    )
    
    # å¼€å§‹åˆ†æ
    qualified_results = analyzer.analyze_top_traded_tokens(
        preset_name=PRESET_NAME,
        max_tokens=MAX_TOKENS,
        delay_between_tokens=DELAY_BETWEEN_TOKENS
    )
    
    main_logger.info(f"ğŸ¯ åˆ†æå®Œæˆï¼å…±æ‰¾åˆ° {len(qualified_results)} ä¸ªç¬¦åˆæ¡ä»¶çš„ä»£å¸")


if __name__ == "__main__":
    main()
