#!/usr/bin/env python3
"""
çƒ­é—¨äº¤æ˜“ä»£å¸æŒæœ‰è€…åˆ†ææ¨¡å—
Top Traded Token Holder Analysis Module

è¾“å…¥: ä½¿ç”¨Jupiteré¢„è®¾é…ç½®çˆ¬å–çƒ­é—¨äº¤æ˜“ä»£å¸
è¾“å‡º: æŒ‰ç…§tokenHolderAnalysisé€ä¸ªè¾“å‡ºç¬¦åˆæ¡ä»¶çš„ä»£å¸æŠ¥å‘Š
ç­›é€‰æ¡ä»¶: ç›®æ ‡ä»£å¸å¤§æˆ·è‡³å°‘5ä¸ªåœ°å€æŒæœ‰åŒä¸€ä¸ªä»£å¸ï¼ˆé™¤äº†ç›®æ ‡ä»£å¸ã€ç¨³å®šå¸ã€SOLï¼‰ï¼Œè¯¥ä»£å¸æŒæœ‰è€…çš„æ€»ä»·å€¼è¶…è¿‡10ä¸‡ç¾å…ƒ

Input: Use Jupiter preset configuration to crawl top traded tokens
Output: Output qualified token reports one by one according to tokenHolderAnalysis
Filter condition: Target token holders must have at least 5 addresses holding the same token (excluding target token, stablecoins, SOL), with total value exceeding $100,000
"""

import sys
import os
import time
from datetime import datetime
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from crawlers.jupiter.topTradedTokens import JupiterTopTradedCrawler
from functions.tokenHolderAnalysis import TokenHolderAnalyzer
from settings.config_manager import ConfigManager


class TopTradedTokenHolderAnalyzer:
    """çƒ­é—¨äº¤æ˜“ä»£å¸æŒæœ‰è€…åˆ†æå™¨"""
    
    def __init__(self, performance_mode: str = 'high_speed'):
        """åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            performance_mode: æ€§èƒ½æ¨¡å¼ ('conservative', 'balanced', 'high_speed', 'lightweight')
        """
        self.performance_mode = performance_mode
        self.config = ConfigManager()
        self.jupiter_crawler = JupiterTopTradedCrawler()
        self.holder_analyzer = TokenHolderAnalyzer(performance_mode=performance_mode)
        
        # æ›´ä¸¥æ ¼çš„ç­›é€‰æ¡ä»¶
        self.min_holders = 5  # è‡³å°‘5ä¸ªåœ°å€æŒæœ‰
        self.min_total_value = 300000  # æ€»ä»·å€¼è¶…è¿‡30ä¸‡ç¾å…ƒ
        
        # ä¸»æµç¨³å®šå¸å’ŒSOLåœ°å€ï¼Œç­›é€‰æ—¶æ’é™¤
        self.excluded_tokens = {
            "So11111111111111111111111111111111111111112",  # SOL
            "EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v",  # USDC
            "Es9vMFrzaCERmJfrF4H2FYD4KCoNkY11McCe8BenwNYB",  # USDT
        }
        
    def set_auth(self, cookie: str, fp_token: str, verify_sign: str, 
                 verify_token: str, dev_id: str, site_info: str):
        """è®¾ç½®OKXè®¤è¯ä¿¡æ¯"""
        self.holder_analyzer.set_auth(cookie, fp_token, verify_sign, 
                                    verify_token, dev_id, site_info)
    
    def analyze_top_traded_tokens(self, preset_name: str = 'lowCapGem_24h',
                                max_tokens: int = 20,
                                delay_between_tokens: float = 2.0) -> List[Dict[str, Any]]:
        """
        åˆ†æçƒ­é—¨äº¤æ˜“ä»£å¸çš„æŒæœ‰è€…
        
        Args:
            preset_name: Jupiteré¢„è®¾åç§°ï¼Œé»˜è®¤ä¸º'lowCapGem_24h'
            max_tokens: æœ€å¤§åˆ†æä»£å¸æ•°é‡
            delay_between_tokens: ä»£å¸åˆ†æé—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            ç¬¦åˆæ¡ä»¶çš„åˆ†æç»“æœåˆ—è¡¨
        """
        print(f"ğŸš€ å¼€å§‹çƒ­é—¨äº¤æ˜“ä»£å¸æŒæœ‰è€…åˆ†æ")
        print(f"ğŸ“Š ä½¿ç”¨é¢„è®¾: {preset_name}")
        print(f"ğŸ”§ æ€§èƒ½æ¨¡å¼: {self.performance_mode}")
        print(f"ğŸ“‹ ç­›é€‰æ¡ä»¶: è‡³å°‘{self.min_holders}ä¸ªåœ°å€æŒæœ‰ï¼Œæ€»ä»·å€¼>${self.min_total_value:,}ï¼ˆæ’é™¤SOLã€USDCã€USDTç­‰ä¸»æµå¸ï¼‰")
        print("="*60)
        
        # 1. è·å–çƒ­é—¨äº¤æ˜“ä»£å¸
        try:
            tokens = self.jupiter_crawler.crawl_with_preset(preset_name)
            print(f"âœ… è·å–åˆ° {len(tokens)} ä¸ªçƒ­é—¨äº¤æ˜“ä»£å¸")
        except Exception as e:
            print(f"âŒ è·å–çƒ­é—¨äº¤æ˜“ä»£å¸å¤±è´¥: {e}")
            return []
        
        if not tokens:
            print("âŒ æœªè·å–åˆ°ä»»ä½•ä»£å¸æ•°æ®")
            return []
        
        # é™åˆ¶åˆ†ææ•°é‡
        tokens_to_analyze = tokens[:max_tokens]
        print(f"ğŸ“ å°†åˆ†æå‰ {len(tokens_to_analyze)} ä¸ªä»£å¸")
        print("="*60)
        
        # 2. é€ä¸ªåˆ†æä»£å¸æŒæœ‰è€…
        qualified_results = []
        
        for i, token in enumerate(tokens_to_analyze, 1):
            try:
                print(f"\nğŸ” [{i}/{len(tokens_to_analyze)}] åˆ†æä»£å¸: {token.symbol} ({token.contract_address})")
                print(f"ğŸ’° å¸‚å€¼: ${token.market_cap:,.0f}" if hasattr(token, 'market_cap') and token.market_cap else "ğŸ’° å¸‚å€¼: æœªçŸ¥")
                
                # åˆ†ææŒæœ‰è€…
                analysis_result = self.holder_analyzer.analyze_token_holders(
                    token.contract_address,
                    chain_id="501"
                )
                
                if "error" in analysis_result:
                    print(f"    âŒ åˆ†æå¤±è´¥: {analysis_result['error']}")
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ç¬¦åˆæ¡ä»¶
                if self._check_qualification(analysis_result, token):
                    print(f"    âœ… ç¬¦åˆæ¡ä»¶ï¼æ·»åŠ åˆ°ç»“æœåˆ—è¡¨")
                    
                    # æ·»åŠ ä»£å¸åŸºæœ¬ä¿¡æ¯
                    analysis_result['token_info'] = {
                        'symbol': token.symbol,
                        'name': token.name,
                        'contract_address': token.contract_address,
                        'market_cap': getattr(token, 'market_cap', None),
                        'volume_24h': getattr(token, 'volume_24h', None),
                        'price': getattr(token, 'price', None)
                    }
                    
                    qualified_results.append(analysis_result)
                    
                    # ç”Ÿæˆå¹¶è¾“å‡ºæŠ¥å‘Š
                    self._output_token_report(analysis_result, token)
                    
                else:
                    print(f"    âŒ ä¸ç¬¦åˆæ¡ä»¶")
                
                # å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                if i < len(tokens_to_analyze):
                    print(f"    â±ï¸ ç­‰å¾… {delay_between_tokens} ç§’...")
                    time.sleep(delay_between_tokens)
                    
            except Exception as e:
                print(f"    âŒ åˆ†æä»£å¸ {token.symbol} æ—¶å‡ºé”™: {e}")
                continue
        
        # 3. è¾“å‡ºæ€»ç»“
        self._output_summary(qualified_results, len(tokens_to_analyze))
        
        return qualified_results
    
    def _check_qualification(self, analysis_result: Dict[str, Any], token) -> bool:
        """
        æ£€æŸ¥åˆ†æç»“æœæ˜¯å¦ç¬¦åˆç­›é€‰æ¡ä»¶
        
        Args:
            analysis_result: åˆ†æç»“æœ
            token: ä»£å¸ä¿¡æ¯
            
        Returns:
            æ˜¯å¦ç¬¦åˆæ¡ä»¶
        """
        common_holdings = analysis_result.get('common_holdings', {})
        top_tokens = common_holdings.get('top_common_tokens', {})
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»£å¸æ»¡è¶³æ¡ä»¶ï¼ˆé™¤äº†ç›®æ ‡ä»£å¸æœ¬èº«ï¼‰
        target_address = analysis_result['token_address']
        
        for token_addr, token_info in top_tokens.items():
            # è·³è¿‡ç›®æ ‡ä»£å¸æœ¬èº«
            if token_addr == target_address:
                continue
            
            # è·³è¿‡ä¸»æµç¨³å®šå¸å’ŒSOL
            if token_addr in self.excluded_tokens:
                continue
                
            holder_count = token_info['holder_count']
            total_value = token_info['total_value']
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ¡ä»¶ï¼šè‡³å°‘5ä¸ªåœ°å€æŒæœ‰ï¼Œæ€»ä»·å€¼è¶…è¿‡50ä¸‡ç¾å…ƒ
            if holder_count >= self.min_holders and total_value >= self.min_total_value:
                print(f"    ğŸ“Š ç¬¦åˆæ¡ä»¶çš„ä»£å¸: {token_info.get('symbol', 'Unknown')} ({holder_count}äººæŒæœ‰, ${total_value:,.0f})")
                return True
        
        print(f"    ğŸ“Š æ— ç¬¦åˆæ¡ä»¶çš„å…±åŒæŒä»“ (éœ€è¦: â‰¥{self.min_holders}äººæŒæœ‰ ä¸” â‰¥${self.min_total_value:,}ï¼Œæ’é™¤ä¸»æµå¸)")
        return False
    
    def _output_token_report(self, analysis_result: Dict[str, Any], token) -> None:
        """è¾“å‡ºå•ä¸ªä»£å¸çš„åˆ†ææŠ¥å‘Š"""
        print("\n" + "="*60)
        print(f"ğŸ¯ {token.symbol} æŒæœ‰è€…åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = self.holder_analyzer.generate_detective_report(
            analysis_result, 
            token.symbol,
            top_holdings_count=15  # æ˜¾ç¤ºå‰15ä¸ªæŒä»“
        )
        
        print(report)
        print("="*60)
        
        # ä¿å­˜åˆ†æç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"toptraded_{token.symbol}_{timestamp}.yaml"
        self.holder_analyzer.save_analysis_result(analysis_result, filename)
    
    def _output_summary(self, qualified_results: List[Dict[str, Any]], total_analyzed: int) -> None:
        """è¾“å‡ºåˆ†ææ€»ç»“"""
        print("\n" + "ğŸ‰"*20)
        print("ğŸ“Š åˆ†ææ€»ç»“")
        print("ğŸ‰"*20)
        print(f"âœ… ç¬¦åˆæ¡ä»¶çš„ä»£å¸: {len(qualified_results)}/{total_analyzed}")
        print(f"ğŸ“‹ ç­›é€‰æ ‡å‡†: è‡³å°‘{self.min_holders}ä¸ªåœ°å€æŒæœ‰åŒä¸€ä»£å¸ï¼Œæ€»ä»·å€¼â‰¥${self.min_total_value:,}ï¼ˆæ’é™¤SOLã€USDCã€USDTç­‰ä¸»æµå¸ï¼‰")
        
        if qualified_results:
            print("\nğŸ† ç¬¦åˆæ¡ä»¶çš„ä»£å¸åˆ—è¡¨:")
            for i, result in enumerate(qualified_results, 1):
                token_info = result.get('token_info', {})
                symbol = token_info.get('symbol', 'Unknown')
                contract = token_info.get('contract_address', '')
                print(f"  {i}. {symbol} ({contract[:8]}...)")
        else:
            print("\nâŒ æœ¬æ¬¡åˆ†æä¸­æ²¡æœ‰ä»£å¸ç¬¦åˆæ¡ä»¶")
        
        print("ğŸ‰"*20)


def main():
    """ä¸»å‡½æ•°"""
    # ğŸ”§ é…ç½®å‚æ•°
    PRESET_NAME = 'lowCapGem_24h'  # Jupiteré¢„è®¾åç§°
    PERFORMANCE_MODE = 'high_speed'   # æ€§èƒ½æ¨¡å¼
    MAX_TOKENS = 1000                 # æœ€å¤§åˆ†æä»£å¸æ•°
    DELAY_BETWEEN_TOKENS = 1.0      # ä»£å¸é—´å»¶è¿Ÿï¼ˆç§’ï¼‰
    
    print(f"ğŸ”§ é…ç½®å‚æ•°:")
    print(f"   é¢„è®¾åç§°: {PRESET_NAME}")
    print(f"   æ€§èƒ½æ¨¡å¼: {PERFORMANCE_MODE}")
    print(f"   æœ€å¤§åˆ†ææ•°: {MAX_TOKENS}")
    print(f"   é—´éš”æ—¶é—´: {DELAY_BETWEEN_TOKENS}ç§’")
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = TopTradedTokenHolderAnalyzer(performance_mode=PERFORMANCE_MODE)
    
    # è®¾ç½®OKXè®¤è¯ä¿¡æ¯ï¼ˆéœ€è¦æ›¿æ¢ä¸ºçœŸå®æ•°æ®ï¼‰
    analyzer.set_auth(
        cookie="devId=01980a38-038a-44d9-8da3-a8276bbcb5b9; ok_site_info===QfxojI5RXa05WZiwiIMFkQPx0Rfh1SPJiOiUGZvNmIsICUKJiOi42bpdWZyJye; locale=en_US; ok_prefer_currency=0%7C1%7Cfalse%7CUSD%7C2%7C%24%7C1%7C1%7CUSD; ok_prefer_udColor=0; ok_prefer_udTimeZone=0; fingerprint_id=01980a38-038a-44d9-8da3-a8276bbcb5b9; first_ref=https%3A%2F%2Fweb3.okx.com%2Ftoken%2Fsolana%2FFbGsCHv8qPvUdmomVAiG72ET5D5kgBJgGoxxfMZipump; ok_global={%22g_t%22:2}; _gcl_au=1.1.1005719754.1755091396; connectedWallet=1; _gid=GA1.2.950489538.1757092345; mse=nf=8|se=0; __cf_bm=KlSlz4ToD2eBrbV2YMpvOTgZSH9aJx8ZbSpNehX__70-1757227578-1.0.1.1-CVB_X0rNpOfUw.n3YJgAepDber7b9fzyAdFONBE5xbbJ9uokVrU0D0ZnKpCgKqWRX9MNMHAODFPNpxZDZYUw1XLYVw6RbsONqf7J5SbrKAc; ok-exp-time=1757227583876; okg.currentMedia=md; tmx_session_id=g42rqe6lkgv_1757227586034; connected=1; fp_s=0; traceId=2130772279702400005; _gat_UA-35324627-3=1; _ga=GA1.1.2083537763.1750302376; _ga_G0EKWWQGTZ=GS2.1.s1757227595$o127$g1$t1757227972$j58$l0$h0; ok-ses-id=ic8FZdwDJ9iztku9zy3wjshp7WSUVWnCq6wpmGltOew4BJU1wkFkGYHyg2jS3JIKpZCB7dnA0g1BCrndYsGLeFEXC9fKYuWwNU4qCZlHwpNQI42XTE4EYPY03Z1p2MaR; _monitor_extras={\"deviceId\":\"KmpeI8VVHan-2zL3_DbOJB\",\"eventId\":6313,\"sequenceNumber\":6313}",
        fp_token="eyJraWQiOiIxNjgzMzgiLCJhbGciOiJFUzI1NiJ9.eyJpYXQiOjE3NTcyMjc1ODksImVmcCI6Ikw1bHcvc0JPNENzV040MXB4MVlTLzRUSjBQaDViZlg0QjhiMTQrTGlHK21NdEt6WWx4TWpFdi9yK0o2MXlCcnIiLCJkaWQiOiIwMTk4MGEzOC0wMzhhLTQ0ZDktOGRhMy1hODI3NmJiY2I1YjkiLCJjcGsiOiJNRmt3RXdZSEtvWkl6ajBDQVFZSUtvWkl6ajBEQVFjRFFnQUVGOGE4RnFDNElLWDJxSHFaOHhaamJnd3BiMDloU2VCSWdxSkdjZ1FEWng0SEp2Z1lIN0g3NE5QblZsRHFWWWNUR0VBWm41aUw4bWdEQTVKbjY5SHJ5Zz09In0.Z1TlLWz0sQ3TvPxo6czcnmZjw1oQ20rvscyTT0CNCx3_e9zPa2WJrfppdAlJ5GbrqqEwyfI2upOd-fy2UGFnSg",
        verify_sign="z0wcDnWum9Gxbbxbq+G6gvmUd7xATTa7V+XX5HvXEe4=",
        verify_token="ac90bf8e-b5fc-4643-a441-2d7b7eb08634",
        dev_id="01980a38-038a-44d9-8da3-a8276bbcb5b9",
        site_info="==QfxojI5RXa05WZiwiIMFkQPx0Rfh1SPJiOi42bpdWZyJye"
    )
    
    # å¼€å§‹åˆ†æ
    qualified_results = analyzer.analyze_top_traded_tokens(
        preset_name=PRESET_NAME,
        max_tokens=MAX_TOKENS,
        delay_between_tokens=DELAY_BETWEEN_TOKENS
    )
    
    print(f"\nğŸ¯ åˆ†æå®Œæˆï¼å…±æ‰¾åˆ° {len(qualified_results)} ä¸ªç¬¦åˆæ¡ä»¶çš„ä»£å¸")


if __name__ == "__main__":
    main()